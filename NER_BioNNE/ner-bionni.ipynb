{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:51.818888Z",
     "iopub.status.busy": "2025-09-07T13:11:51.818368Z",
     "iopub.status.idle": "2025-09-07T13:11:52.080872Z",
     "shell.execute_reply": "2025-09-07T13:11:52.080166Z",
     "shell.execute_reply.started": "2025-09-07T13:11:51.818865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BioNNE-L Shared Task dataset is a biomedical natural language processing resource designed for Named Entity Recognition (NER) and relation extraction.\n",
    "It contains scientific and clinical texts annotated with biomedical entities (e.g., genes, proteins, chemicals, diseases) and their relationships, making it useful for training and evaluating models in biomedical text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.086570Z",
     "iopub.status.busy": "2025-09-07T13:11:52.086354Z",
     "iopub.status.idle": "2025-09-07T13:11:52.256581Z",
     "shell.execute_reply": "2025-09-07T13:11:52.255896Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.086551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev  test  train\n"
     ]
    }
   ],
   "source": [
    "!ls '/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/texts/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.257655Z",
     "iopub.status.busy": "2025-09-07T13:11:52.257411Z",
     "iopub.status.idle": "2025-09-07T13:11:52.261561Z",
     "shell.execute_reply": "2025-09-07T13:11:52.260948Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.257617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "directory_texts =  '/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/texts/en/train'\n",
    "file_annotations = '/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/tsv/en/bionnel_en_train.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the structure is as follows :\n",
    "##### file_annotations this is a .csv file that i printed bellow it has the image name (id) and the text (word or sentence) , its type and the span from where to where the span is essential in training for ner\n",
    "##### directory_texts this has files each file is a document_id in file_annotations so the text and spans in this file are declared in file_annotations , so we store in a dictionnary the txext as the vaalue and the image id as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.263831Z",
     "iopub.status.busy": "2025-09-07T13:11:52.263415Z",
     "iopub.status.idle": "2025-09-07T13:11:52.714349Z",
     "shell.execute_reply": "2025-09-07T13:11:52.713808Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.263808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id     text entity_type      spans  UMLS_CUI\n",
      "0  25591652_en  Seizure        DISO    976-983  C0949003\n",
      "1  25591652_en     AEDs        CHEM  1187-1191  C0887457\n",
      "2  25591652_en     AEDs        CHEM  1849-1853  C0887457\n",
      "3  25591652_en     AEDs        CHEM  2033-2037  C0887457\n",
      "4  25591652_en     AEDs        CHEM  1654-1658  C0887457\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_annotations , sep=\"\\t\")\n",
    "print(df.head())\n",
    "\n",
    "id_to_text = {}\n",
    "\n",
    "for file in os.listdir(directory_texts):\n",
    "    file_id = file.split(\".\")[0]\n",
    "    with open(os.path.join(directory_texts , file ), \"r\") as f:\n",
    "        id_to_text[file_id] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the first 100 caracters of the image \"25842921_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.715196Z",
     "iopub.status.busy": "2025-09-07T13:11:52.714926Z",
     "iopub.status.idle": "2025-09-07T13:11:52.720794Z",
     "shell.execute_reply": "2025-09-07T13:11:52.720106Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.715171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Functional morphology of ischemic cardiomyopathy]. OBJECTIVE To show that ischemic cardiomyopathy ('"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_text[\"25842921_en\"][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the dataframe grouped by the document id so the result is ofr each document id it groups the entities type and the span of each entity\n",
    "\n",
    "#### then we get the text for each document id and we iterate through this group and store the spans and the labels then we append these three values in train data in the way that the pipe NER expects (entities)  text, {\"entities\": entities}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.721818Z",
     "iopub.status.busy": "2025-09-07T13:11:52.721595Z",
     "iopub.status.idle": "2025-09-07T13:11:52.853991Z",
     "shell.execute_reply": "2025-09-07T13:11:52.853481Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.721801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for doc_id, group in df.groupby(\"document_id\"):\n",
    "    if doc_id not in id_to_text:\n",
    "        continue\n",
    "\n",
    "    text = id_to_text[doc_id]\n",
    "    entities = []\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            # sometimes spans are like '476,492-500'\n",
    "            spans = row[\"spans\"].split(\",\")  # split multiple spans\n",
    "            for sp in spans:\n",
    "                start_end = sp.split(\"-\")\n",
    "                if len(start_end) != 2:\n",
    "                    print(f\"Skipping invalid span {sp}\")\n",
    "                    continue\n",
    "                start, end = int(start_end[0]), int(start_end[1])\n",
    "                label = row[\"entity_type\"]\n",
    "\n",
    "                span_text = text[start:end]\n",
    "                # optional sanity check\n",
    "                #print(doc_id, span_text, \"->\", label)\n",
    "\n",
    "                entities.append((start, end, label))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping row {row} due to {e}\")\n",
    "\n",
    "    train_data.append((text, {\"entities\": entities}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a test now data is ready ready so the rest is call the model and perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.854744Z",
     "iopub.status.busy": "2025-09-07T13:11:52.854560Z",
     "iopub.status.idle": "2025-09-07T13:11:52.858888Z",
     "shell.execute_reply": "2025-09-07T13:11:52.858287Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.854728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Impact of bosentan therapy on stress-induced pulmonary hypertension in patients with systemic sclerosis]. \\nAIM To describe hemodynamic and clinical changes in patients with elevated mean pulmonary artery pressure (MPAP) > 30 mm Hg during exercise and the impact of bosentan therapy on stress-induced pulmonary hypertension (SIPH).  \\nSUBJECTS AND METHODS The study included 19 patients with systemic sclerosis (SDS) in whom possible causes of pulmonary hypertension (PH) (lung and left heart injuries and thromboembolism) were excluded. \\nAll the patients underwent pulmonary artery catheterization at rest and during exercise.  \\nThe hemodynamic (right atrial pressure (RAP), systolic and diastolic pressure, MPAP, pulmonary artery wedge pressure (PAWP), cardiac output (CO) by a thermodilution technique), clinical (demographic, immunological, and instrumental) parameters were analyzed and the risk of pulmonary arterial hypertension (PAH) was also calculated; 5 patients with SIPH received 16-week bosentan therapy according to the conventional regimen. \\nRESULTS Ten of the 19 patients were at increased risk for PAH in accordance with the DETECT scale, but no signs of PH at resting catheterization were found in anybody. \\nIn 5 patients, MPAP, was in the range from 21 to 24 mm Hg; in 9 (47%) patients were found to have SIPH, a median MPAP of 35 (32; 41) mm Hg.  \\nSeven patients had no diagnostic changes during exercise; 3 patients could not perform an exercise test. \\nThere were correlations between MPAP and DETECT risk scores (p < 0.05). \\nThe patients with SIPH had significantly higher levels of resting MPAP and exercise pulmonary vascular resistance (PVR) and PAWP. \\nThe calculated DETECT risk was significantly higher in the SIPH group. \\nThe level of uric acid was also higher in the SIPH group (p < 0.05). \\nThere were no changes in NT-proBNP levels, telangiectasias and anti-centromere antibodies, and EchoCG and lung test results. \\nDuring 16-week bosentan therapy, there was a significant decrease in MPAP and transpulmonary gradient during exercise, but PVR, MPAP/CO ratio and NT-proBNP levels tended to decrease. \\nCONCLUSION In the patients with SDS, SIPH may be a stage of pulmonary vasculopathy that precedes the development of clinical PAH. \\nThe use of current PAH-specific drugs used at the preclinical stage of the disease may substantially improve lifetime prognosis in patients with SDS-associated PAH. \\n', {'entities': [(56, 68, 'DISO'), (453, 465, 'DISO'), (2190, 2199, 'ANATOMY'), (1631, 1640, 'ANATOMY'), (481, 500, 'DISO'), (391, 409, 'DISO'), (486, 500, 'DISO'), (86, 104, 'DISO'), (486, 491, 'ANATOMY'), (978, 982, 'DISO'), (913, 921, 'ANATOMY'), (400, 409, 'DISO'), (2200, 2212, 'DISO'), (754, 761, 'ANATOMY'), (922, 934, 'DISO'), (95, 104, 'DISO'), (1000, 1008, 'CHEM'), (266, 274, 'CHEM'), (11, 19, 'CHEM'), (2255, 2258, 'DISO'), (2406, 2424, 'DISO'), (2167, 2171, 'DISO'), (1324, 1328, 'DISO'), (1763, 1772, 'CHEM'), (1737, 1741, 'DISO'), (1899, 1909, 'CHEM'), (2406, 2409, 'DISO'), (2162, 2165, 'DISO'), (411, 414, 'DISO'), (724, 730, 'ANATOMY'), (903, 934, 'DISO'), (325, 329, 'DISO'), (198, 204, 'ANATOMY'), (575, 581, 'ANATOMY'), (936, 939, 'DISO'), (652, 658, 'ANATOMY'), (1115, 1118, 'DISO'), (1631, 1649, 'ANATOMY'), (2421, 2424, 'DISO'), (1768, 1772, 'CHEM'), (472, 476, 'DISO'), (492, 500, 'DISO'), (2280, 2283, 'DISO'), (1565, 1569, 'DISO'), (714, 723, 'ANATOMY'), (1641, 1649, 'ANATOMY'), (2280, 2298, 'CHEM'), (286, 323, 'DISO'), (2092, 2101, 'CHEM'), (1961, 1969, 'CHEM'), (1845, 1854, 'CHEM'), (31, 68, 'DISO'), (1883, 1909, 'CHEM'), (301, 323, 'DISO'), (903, 912, 'ANATOMY'), (2190, 2212, 'DISO'), (188, 204, 'ANATOMY'), (481, 491, 'ANATOMY'), (903, 921, 'ANATOMY'), (311, 323, 'DISO'), (467, 469, 'DISO'), (1172, 1174, 'DISO'), (646, 658, 'ANATOMY'), (1796, 1800, 'DISO'), (714, 730, 'ANATOMY'), (565, 581, 'ANATOMY'), (46, 68, 'DISO'), (443, 465, 'DISO'), (301, 310, 'ANATOMY'), (1926, 1930, 'ANATOMY'), (505, 520, 'DISO'), (443, 452, 'ANATOMY'), (472, 476, 'ANATOMY'), (188, 197, 'ANATOMY'), (46, 55, 'ANATOMY'), (565, 574, 'ANATOMY')]})\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "library that has the models of ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:11:52.860437Z",
     "iopub.status.busy": "2025-09-07T13:11:52.859699Z",
     "iopub.status.idle": "2025-09-07T13:12:02.710924Z",
     "shell.execute_reply": "2025-09-07T13:12:02.710356Z",
     "shell.execute_reply.started": "2025-09-07T13:11:52.860410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the model we'll use anglish_core_web_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:02.712073Z",
     "iopub.status.busy": "2025-09-07T13:12:02.711740Z",
     "iopub.status.idle": "2025-09-07T13:12:11.722188Z",
     "shell.execute_reply": "2025-09-07T13:12:11.721211Z",
     "shell.execute_reply.started": "2025-09-07T13:12:02.712056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:11.723945Z",
     "iopub.status.busy": "2025-09-07T13:12:11.723482Z",
     "iopub.status.idle": "2025-09-07T13:12:13.309820Z",
     "shell.execute_reply": "2025-09-07T13:12:13.309087Z",
     "shell.execute_reply.started": "2025-09-07T13:12:11.723907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the pipes ner is already a pipe as you can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.310895Z",
     "iopub.status.busy": "2025-09-07T13:12:13.310647Z",
     "iopub.status.idle": "2025-09-07T13:12:13.316059Z",
     "shell.execute_reply": "2025-09-07T13:12:13.315293Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.310871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we keep the pip we need which is ner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.317076Z",
     "iopub.status.busy": "2025-09-07T13:12:13.316848Z",
     "iopub.status.idle": "2025-09-07T13:12:13.354292Z",
     "shell.execute_reply": "2025-09-07T13:12:13.353622Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.317061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ner = nlp(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i printed the entities that pipe \"ner\" can already recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.357508Z",
     "iopub.status.busy": "2025-09-07T13:12:13.357295Z",
     "iopub.status.idle": "2025-09-07T13:12:13.362778Z",
     "shell.execute_reply": "2025-09-07T13:12:13.362020Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.357493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels[\"ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i created it from scratch and add it to nlp pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.363893Z",
     "iopub.status.busy": "2025-09-07T13:12:13.363622Z",
     "iopub.status.idle": "2025-09-07T13:12:13.539992Z",
     "shell.execute_reply": "2025-09-07T13:12:13.539223Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.363877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")  # start from scratch\n",
    "ner = nlp.add_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in here i added the entities of my data that are stored in train_data to ner that i've just created from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.541026Z",
     "iopub.status.busy": "2025-09-07T13:12:13.540789Z",
     "iopub.status.idle": "2025-09-07T13:12:13.554573Z",
     "shell.execute_reply": "2025-09-07T13:12:13.553988Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.541006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ ,annotations in train_data:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "len(ner.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store in other_pipes all the pipes in nlp except ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:13.555715Z",
     "iopub.status.busy": "2025-09-07T13:12:13.555253Z",
     "iopub.status.idle": "2025-09-07T13:12:13.559585Z",
     "shell.execute_reply": "2025-09-07T13:12:13.558900Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.555694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a function that remooves everlaps like :\n",
    "##### if we have a span of a word from [122-128] and another soan from [125-130] this is called overlaps which incorrect we can't have two words merged or it's a problem while creating the dataset from the owner so we remove this spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:48.137305Z",
     "iopub.status.busy": "2025-09-07T13:12:48.136471Z",
     "iopub.status.idle": "2025-09-07T13:12:48.145677Z",
     "shell.execute_reply": "2025-09-07T13:12:48.144809Z",
     "shell.execute_reply.started": "2025-09-07T13:12:48.137271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "    filtered = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:\n",
    "            filtered.append((start, end, label))\n",
    "            last_end = end\n",
    "    return filtered\n",
    "\n",
    "train_data_fixed = []\n",
    "for text, ann in train_data:\n",
    "    entities = remove_overlaps(ann[\"entities\"])\n",
    "    train_data_fixed.append((text, {\"entities\": entities}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this function we just do a simple cleaning from numbers and special caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:12:49.944954Z",
     "iopub.status.busy": "2025-09-07T13:12:49.944666Z",
     "iopub.status.idle": "2025-09-07T13:12:49.950354Z",
     "shell.execute_reply": "2025-09-07T13:12:49.949587Z",
     "shell.execute_reply.started": "2025-09-07T13:12:49.944931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_span(span_str):\n",
    "    # Remove commas or other non-digit characters\n",
    "    span_str = re.sub(r\"[^\\d\\-]\", \"\", span_str)\n",
    "    if \"-\" not in span_str:\n",
    "        return None\n",
    "    start, end = span_str.split(\"-\", 1)\n",
    "    try:\n",
    "        return int(start), int(end)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# 4️⃣ Function to remove overlapping entities\n",
    "def remove_overlaps(entities):\n",
    "    # Sort by start position\n",
    "    entities = sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "    clean_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # keep non-overlapping\n",
    "            clean_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return clean_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part bellow is the training first we disable all the other pipes in nlp we keep only \"ner\" then we initialize the weights of the optimizer i did 150 eochs it gave me the mowest loss and for each epochs we shuffle data to make the model learns better also i added a minibath to increase speed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:13:30.760846Z",
     "iopub.status.busy": "2025-09-07T13:13:30.760291Z",
     "iopub.status.idle": "2025-09-07T13:17:28.894291Z",
     "shell.execute_reply": "2025-09-07T13:17:28.893545Z",
     "shell.execute_reply.started": "2025-09-07T13:13:30.760820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations preview:\n",
      "    document_id     text entity_type      spans  UMLS_CUI\n",
      "0  25591652_en  Seizure        DISO    976-983  C0949003\n",
      "1  25591652_en     AEDs        CHEM  1187-1191  C0887457\n",
      "2  25591652_en     AEDs        CHEM  1849-1853  C0887457\n",
      "3  25591652_en     AEDs        CHEM  2033-2037  C0887457\n",
      "4  25591652_en     AEDs        CHEM  1654-1658  C0887457\n",
      "Number of docs loaded: 54\n",
      "Total training examples: 54\n",
      "Example: ('Objective.\\nTo analyze epileptic seizure aggravation associated with antiepileptic drugs (AED) in adult patients.\\nMaterial and methods.\\nWe examined 1407 patients aged 18-89 years. \\nDifferent patterns of seizure aggravation were identified in 103 patients. \\nResults.\\nAggravated seizures due the generic substitution were found in 32 patients. \\nFirst was topiramate (TPM) (n=12), followed by valproates (VPA) (n=8), carbamazepine (CBZ) (n=5), lamotrigine (LTG) (n=1) and levetiracetam (LEV) (n=1). \\nPatients with idiopathic generalized epilepsies (IGE) suffered aggravation with CBZ in 17 cases, VPA - in 6, TPM - in 6, LTG - in 1 and LEV - in 1. \\nCBZ aggravated absences in patients with juvenile absence epilepsy (JAE) were found in 5 cases of 17 (29.4%), with childhood absence epilepsy (CAE) - in 1 of 24 (4.2%), absences and myoclonic jerks in juvenile myoclonic epilepsy (JME) - in 9 of 47 (19.1%), absences with eyelid myoclonus (Jeavons syndrome) - in 2 of 11 (18.2%).  \\nSeizure aggravations with different AEDs were observed in 13 patients. \\nThis pattern of aggravation was associated with resistant epilepsy and poor prognosis.  \\nSeizure aggravation due to increasing the dose of AEDs was found in 10 cases and associated with resistant epilepsy and poor outcome. \\n\"True\" aggravation was associated with CBZ in 34 patients, including 16 IGE patients, with TPM - in 13, VPA - 5, LTG - 5, LEV - 2 and with clonazepam in 1 patient. \\nMost often seizure aggravation was associated with CBZ in IGE patients (8.3%), and with LTG, TPM and CBZ in patients with other epileptic syndromes (non IGE): 4.9%, 4% and 3.7%, respectively. \\nConclusions. \\nDifferent AEDs can cause aggravation. \\nSeizures aggravation after generic substitution was characteristic of TPM most frequently. \\nSeizure aggravation with different AEDs and due to increasing the dose of AEDs was associated with poor outcome. \\nIn all cases, seizure aggravation was more frequently caused by LTG (4.7%), CBZ (3.7%) and TPM (3.3%). \\nTo reduce the risk of aggravation, slow AEDs titration and keeping a diary of seizures are required. \\n', {'entities': [(22, 31, 'DISO'), (32, 39, 'DISO'), (68, 87, 'CHEM'), (89, 92, 'CHEM'), (202, 209, 'DISO'), (276, 284, 'DISO'), (352, 362, 'CHEM'), (364, 367, 'CHEM'), (389, 399, 'CHEM'), (401, 404, 'CHEM'), (413, 426, 'CHEM'), (428, 431, 'CHEM'), (440, 451, 'CHEM'), (453, 456, 'CHEM'), (468, 481, 'CHEM'), (483, 486, 'CHEM'), (510, 543, 'DISO'), (545, 548, 'DISO'), (576, 579, 'CHEM'), (593, 596, 'CHEM'), (605, 608, 'CHEM'), (617, 620, 'CHEM'), (632, 635, 'CHEM'), (645, 648, 'CHEM'), (660, 668, 'DISO'), (686, 711, 'DISO'), (713, 716, 'DISO'), (760, 786, 'DISO'), (788, 791, 'DISO'), (814, 822, 'DISO'), (827, 842, 'DISO'), (846, 873, 'DISO'), (875, 878, 'DISO'), (902, 910, 'DISO'), (916, 922, 'ANATOMY'), (923, 932, 'DISO'), (934, 950, 'DISO'), (976, 983, 'DISO'), (1012, 1016, 'CHEM'), (1096, 1114, 'DISO'), (1137, 1144, 'DISO'), (1187, 1191, 'CHEM'), (1234, 1252, 'DISO'), (1311, 1314, 'CHEM'), (1344, 1347, 'DISO'), (1363, 1366, 'CHEM'), (1376, 1379, 'CHEM'), (1385, 1388, 'CHEM'), (1394, 1397, 'CHEM'), (1411, 1421, 'CHEM'), (1448, 1455, 'DISO'), (1488, 1491, 'CHEM'), (1495, 1498, 'DISO'), (1525, 1528, 'CHEM'), (1530, 1533, 'CHEM'), (1538, 1541, 'CHEM'), (1565, 1584, 'DISO'), (1586, 1593, 'DISO'), (1654, 1658, 'CHEM'), (1683, 1691, 'DISO'), (1753, 1756, 'CHEM'), (1775, 1782, 'DISO'), (1810, 1814, 'CHEM'), (1849, 1853, 'CHEM'), (1903, 1910, 'DISO'), (1953, 1956, 'CHEM'), (1965, 1968, 'CHEM'), (1980, 1983, 'CHEM'), (2033, 2037, 'CHEM'), (2071, 2079, 'DISO')]})\n",
      "Train size: 48, Validation size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-07 13:13:31,706] [INFO] Created vocabulary\n",
      "[2025-09-07 13:13:31,707] [INFO] Finished initializing nlp object\n",
      "/usr/local/lib/python3.11/dist-packages/thinc/layers/layernorm.py:31: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  d_xhat = N * dY - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"[An antinociceptive effect of chondroprotectors: a...\" with entities \"[(30, 47, 'CHEM'), (123, 140, 'CHEM'), (161, 183, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"[Role of the skin expression of neuropeptides, neu...\" with entities \"[(13, 17, 'ANATOMY'), (32, 45, 'CHEM'), (47, 60, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"[Impact of preoperative drug therapy on adhesion m...\" with entities \"[(40, 57, 'CHEM'), (72, 82, 'ANATOMY'), (83, 89, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"[Leptin, resistin, and hormonal and metabolic para...\" with entities \"[(1, 7, 'CHEM'), (9, 17, 'CHEM'), (23, 31, 'CHEM')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"UNLABELLED  The authors present the material of th...\" with entities \"[(118, 148, 'DISO'), (150, 153, 'ANATOMY'), (162, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"AIM  to investigate the specific features of somat...\" with entities \"[(45, 57, 'CHEM'), (58, 66, 'CHEM'), (85, 90, 'ANA...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"[Pulmonary paecilomycosis: Diagnosis and treatment...\" with entities \"[(1, 10, 'ANATOMY'), (11, 25, 'DISO'), (136, 150, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 - Losses: {'ner': 11967.277745723724}\n",
      "Epoch 2/150 - Losses: {'ner': 5528.112783432007}\n",
      "Epoch 3/150 - Losses: {'ner': 3125.5043714297935}\n",
      "Epoch 4/150 - Losses: {'ner': 2931.766532329377}\n",
      "Epoch 5/150 - Losses: {'ner': 2811.7167774140835}\n",
      "Epoch 6/150 - Losses: {'ner': 2736.850614272058}\n",
      "Epoch 7/150 - Losses: {'ner': 2484.6074265446514}\n",
      "Epoch 8/150 - Losses: {'ner': 2336.8233747389168}\n",
      "Epoch 9/150 - Losses: {'ner': 2233.771256131353}\n",
      "Epoch 10/150 - Losses: {'ner': 2087.0974660967477}\n",
      "Epoch 11/150 - Losses: {'ner': 1987.7381681975676}\n",
      "Epoch 12/150 - Losses: {'ner': 1815.5571449412528}\n",
      "Epoch 13/150 - Losses: {'ner': 1772.9244377919463}\n",
      "Epoch 14/150 - Losses: {'ner': 1742.1013987311103}\n",
      "Epoch 15/150 - Losses: {'ner': 1739.0836687487827}\n",
      "Epoch 16/150 - Losses: {'ner': 1554.107002620399}\n",
      "Epoch 17/150 - Losses: {'ner': 1494.991301888164}\n",
      "Epoch 18/150 - Losses: {'ner': 1404.82836344851}\n",
      "Epoch 19/150 - Losses: {'ner': 1384.8287853726015}\n",
      "Epoch 20/150 - Losses: {'ner': 1325.2368144802426}\n",
      "Epoch 21/150 - Losses: {'ner': 1232.715523918589}\n",
      "Epoch 22/150 - Losses: {'ner': 1145.1459744000738}\n",
      "Epoch 23/150 - Losses: {'ner': 1130.0764342774928}\n",
      "Epoch 24/150 - Losses: {'ner': 1153.8187190220121}\n",
      "Epoch 25/150 - Losses: {'ner': 1170.6924846816078}\n",
      "Epoch 26/150 - Losses: {'ner': 1063.2857494442997}\n",
      "Epoch 27/150 - Losses: {'ner': 930.4725954027998}\n",
      "Epoch 28/150 - Losses: {'ner': 908.2280763006191}\n",
      "Epoch 29/150 - Losses: {'ner': 805.5923907790675}\n",
      "Epoch 30/150 - Losses: {'ner': 806.9787996742301}\n",
      "Epoch 31/150 - Losses: {'ner': 708.0598260171837}\n",
      "Epoch 32/150 - Losses: {'ner': 728.9907317505434}\n",
      "Epoch 33/150 - Losses: {'ner': 590.6376739006466}\n",
      "Epoch 34/150 - Losses: {'ner': 563.3709716978663}\n",
      "Epoch 35/150 - Losses: {'ner': 506.6378896511444}\n",
      "Epoch 36/150 - Losses: {'ner': 478.26236590838755}\n",
      "Epoch 37/150 - Losses: {'ner': 471.28909368012614}\n",
      "Epoch 38/150 - Losses: {'ner': 455.4834796023915}\n",
      "Epoch 39/150 - Losses: {'ner': 496.36086749066874}\n",
      "Epoch 40/150 - Losses: {'ner': 430.3325427020449}\n",
      "Epoch 41/150 - Losses: {'ner': 446.9118494406181}\n",
      "Epoch 42/150 - Losses: {'ner': 408.33477924644416}\n",
      "Epoch 43/150 - Losses: {'ner': 382.2655691698398}\n",
      "Epoch 44/150 - Losses: {'ner': 394.65836039048787}\n",
      "Epoch 45/150 - Losses: {'ner': 388.72848007055006}\n",
      "Epoch 46/150 - Losses: {'ner': 401.5271063036789}\n",
      "Epoch 47/150 - Losses: {'ner': 357.13969579690126}\n",
      "Epoch 48/150 - Losses: {'ner': 353.0930176697985}\n",
      "Epoch 49/150 - Losses: {'ner': 325.3475995623264}\n",
      "Epoch 50/150 - Losses: {'ner': 358.90280368874585}\n",
      "Epoch 51/150 - Losses: {'ner': 304.0558486826018}\n",
      "Epoch 52/150 - Losses: {'ner': 269.9684869415268}\n",
      "Epoch 53/150 - Losses: {'ner': 258.9512895222528}\n",
      "Epoch 54/150 - Losses: {'ner': 301.86648786596254}\n",
      "Epoch 55/150 - Losses: {'ner': 226.57158841012318}\n",
      "Epoch 56/150 - Losses: {'ner': 249.88979944095863}\n",
      "Epoch 57/150 - Losses: {'ner': 235.84687984501596}\n",
      "Epoch 58/150 - Losses: {'ner': 228.61451475511538}\n",
      "Epoch 59/150 - Losses: {'ner': 210.45610068911247}\n",
      "Epoch 60/150 - Losses: {'ner': 199.74781827360988}\n",
      "Epoch 61/150 - Losses: {'ner': 248.9490009001121}\n",
      "Epoch 62/150 - Losses: {'ner': 244.18034232271853}\n",
      "Epoch 63/150 - Losses: {'ner': 163.25274786113084}\n",
      "Epoch 64/150 - Losses: {'ner': 160.59667247821784}\n",
      "Epoch 65/150 - Losses: {'ner': 145.28146287977316}\n",
      "Epoch 66/150 - Losses: {'ner': 182.24102301359457}\n",
      "Epoch 67/150 - Losses: {'ner': 137.68579089516732}\n",
      "Epoch 68/150 - Losses: {'ner': 139.48742271150604}\n",
      "Epoch 69/150 - Losses: {'ner': 130.64575244747982}\n",
      "Epoch 70/150 - Losses: {'ner': 155.28497390346206}\n",
      "Epoch 71/150 - Losses: {'ner': 139.67124145645155}\n",
      "Epoch 72/150 - Losses: {'ner': 127.04284175496646}\n",
      "Epoch 73/150 - Losses: {'ner': 99.26481433118303}\n",
      "Epoch 74/150 - Losses: {'ner': 119.67087203729514}\n",
      "Epoch 75/150 - Losses: {'ner': 125.14268018429887}\n",
      "Epoch 76/150 - Losses: {'ner': 99.65579614177395}\n",
      "Epoch 77/150 - Losses: {'ner': 116.07929361116416}\n",
      "Epoch 78/150 - Losses: {'ner': 120.993869846309}\n",
      "Epoch 79/150 - Losses: {'ner': 127.74410507877076}\n",
      "Epoch 80/150 - Losses: {'ner': 108.09682251787144}\n",
      "Epoch 81/150 - Losses: {'ner': 124.78767348811266}\n",
      "Epoch 82/150 - Losses: {'ner': 111.31704468248199}\n",
      "Epoch 83/150 - Losses: {'ner': 129.3650407318744}\n",
      "Epoch 84/150 - Losses: {'ner': 105.84588351812775}\n",
      "Epoch 85/150 - Losses: {'ner': 92.8941514053989}\n",
      "Epoch 86/150 - Losses: {'ner': 86.42568830669582}\n",
      "Epoch 87/150 - Losses: {'ner': 115.00026254121587}\n",
      "Epoch 88/150 - Losses: {'ner': 102.88076086794833}\n",
      "Epoch 89/150 - Losses: {'ner': 114.53520545846385}\n",
      "Epoch 90/150 - Losses: {'ner': 103.30211340501938}\n",
      "Epoch 91/150 - Losses: {'ner': 86.96502806208207}\n",
      "Epoch 92/150 - Losses: {'ner': 79.9422120586553}\n",
      "Epoch 93/150 - Losses: {'ner': 74.96868727338008}\n",
      "Epoch 94/150 - Losses: {'ner': 83.2300879834937}\n",
      "Epoch 95/150 - Losses: {'ner': 83.67021307571031}\n",
      "Epoch 96/150 - Losses: {'ner': 79.61734352210384}\n",
      "Epoch 97/150 - Losses: {'ner': 84.3106018353271}\n",
      "Epoch 98/150 - Losses: {'ner': 91.01567360634058}\n",
      "Epoch 99/150 - Losses: {'ner': 66.4152429951489}\n",
      "Epoch 100/150 - Losses: {'ner': 94.7090881956684}\n",
      "Epoch 101/150 - Losses: {'ner': 67.1985143486726}\n",
      "Epoch 102/150 - Losses: {'ner': 89.40134318922466}\n",
      "Epoch 103/150 - Losses: {'ner': 83.51815406832645}\n",
      "Epoch 104/150 - Losses: {'ner': 77.28759262763845}\n",
      "Epoch 105/150 - Losses: {'ner': 92.09549132966566}\n",
      "Epoch 106/150 - Losses: {'ner': 60.267710651750214}\n",
      "Epoch 107/150 - Losses: {'ner': 72.66251581461793}\n",
      "Epoch 108/150 - Losses: {'ner': 46.543806231598744}\n",
      "Epoch 109/150 - Losses: {'ner': 65.79217498819945}\n",
      "Epoch 110/150 - Losses: {'ner': 62.177147237616396}\n",
      "Epoch 111/150 - Losses: {'ner': 54.43956002812069}\n",
      "Epoch 112/150 - Losses: {'ner': 63.48744437503825}\n",
      "Epoch 113/150 - Losses: {'ner': 53.57792746398515}\n",
      "Epoch 114/150 - Losses: {'ner': 43.553211825569306}\n",
      "Epoch 115/150 - Losses: {'ner': 55.35993072089809}\n",
      "Epoch 116/150 - Losses: {'ner': 56.66499615041031}\n",
      "Epoch 117/150 - Losses: {'ner': 64.9876208871608}\n",
      "Epoch 118/150 - Losses: {'ner': 46.263546299029585}\n",
      "Epoch 119/150 - Losses: {'ner': 58.08198584871686}\n",
      "Epoch 120/150 - Losses: {'ner': 46.066295917770624}\n",
      "Epoch 121/150 - Losses: {'ner': 48.49895391820854}\n",
      "Epoch 122/150 - Losses: {'ner': 37.64264202088103}\n",
      "Epoch 123/150 - Losses: {'ner': 41.399488106276465}\n",
      "Epoch 124/150 - Losses: {'ner': 52.78938066608984}\n",
      "Epoch 125/150 - Losses: {'ner': 45.56911411217111}\n",
      "Epoch 126/150 - Losses: {'ner': 57.812750183924415}\n",
      "Epoch 127/150 - Losses: {'ner': 44.108735050206725}\n",
      "Epoch 128/150 - Losses: {'ner': 47.69573618754129}\n",
      "Epoch 129/150 - Losses: {'ner': 61.82629467695444}\n",
      "Epoch 130/150 - Losses: {'ner': 47.28799133843831}\n",
      "Epoch 131/150 - Losses: {'ner': 46.0421806230722}\n",
      "Epoch 132/150 - Losses: {'ner': 71.89185675819307}\n",
      "Epoch 133/150 - Losses: {'ner': 48.49950484588369}\n",
      "Epoch 134/150 - Losses: {'ner': 44.63062641857677}\n",
      "Epoch 135/150 - Losses: {'ner': 44.09538247977653}\n",
      "Epoch 136/150 - Losses: {'ner': 48.85514306487726}\n",
      "Epoch 137/150 - Losses: {'ner': 41.34737049460058}\n",
      "Epoch 138/150 - Losses: {'ner': 38.18846852199279}\n",
      "Epoch 139/150 - Losses: {'ner': 57.43098610350637}\n",
      "Epoch 140/150 - Losses: {'ner': 42.949642814714515}\n",
      "Epoch 141/150 - Losses: {'ner': 29.514721107774296}\n",
      "Epoch 142/150 - Losses: {'ner': 38.09022887543337}\n",
      "Epoch 143/150 - Losses: {'ner': 39.272503752144985}\n",
      "Epoch 144/150 - Losses: {'ner': 38.086943792189295}\n",
      "Epoch 145/150 - Losses: {'ner': 35.82985725885509}\n",
      "Epoch 146/150 - Losses: {'ner': 49.82950807988752}\n",
      "Epoch 147/150 - Losses: {'ner': 29.9350834059492}\n",
      "Epoch 148/150 - Losses: {'ner': 51.92975303726585}\n",
      "Epoch 149/150 - Losses: {'ner': 37.6661349499294}\n",
      "Epoch 150/150 - Losses: {'ner': 31.368030880042166}\n",
      "Model saved to ./nerel_bio_ner_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 8️⃣ Training loop\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    epochs = 150\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_examples)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.5))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "            nlp.update(examples, drop=0.3, losses=losses)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Losses: {losses}\")\n",
    "\n",
    "# 9️⃣ Save the trained model\n",
    "output_dir = \"./nerel_bio_ner_model\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see above the loss i could reached is 31.  which is husge so we'are expecting bad performance from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:21:34.392050Z",
     "iopub.status.busy": "2025-09-07T13:21:34.391777Z",
     "iopub.status.idle": "2025-09-07T13:21:34.396852Z",
     "shell.execute_reply": "2025-09-07T13:21:34.396151Z",
     "shell.execute_reply.started": "2025-09-07T13:21:34.392032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples[1][1][\"entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this bellow is a test on an example already trained on and the results were very good it detected all the entities and their types \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:22:06.579512Z",
     "iopub.status.busy": "2025-09-07T13:22:06.579036Z",
     "iopub.status.idle": "2025-09-07T13:22:06.604437Z",
     "shell.execute_reply": "2025-09-07T13:22:06.603863Z",
     "shell.execute_reply.started": "2025-09-07T13:22:06.579486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities predicted:\n",
      "skin 13 17 ANATOMY\n",
      "neuropeptides 32 45 CHEM\n",
      "neurotrophins 47 60 CHEM\n",
      "receptors 71 80 CHEM\n",
      "pathogenesis 88 100 DISO\n",
      "dermatoses 104 114 DISO\n",
      "neurotransmitters 149 166 CHEM\n",
      "receptors 177 186 CHEM\n",
      "itch 209 213 DISO\n",
      "skin 242 246 ANATOMY\n",
      "inflammatory response 247 268 DISO\n",
      "psoriasis 286 295 DISO\n",
      "atopic dermatitis 300 317 DISO\n",
      "Skin 341 345 ANATOMY\n",
      "biopsy specimens 346 362 ANATOMY\n",
      "psoriasis 385 394 DISO\n",
      "atopic dermatitis 416 433 DISO\n",
      "protein 575 582 CHEM\n",
      "PGP9.5 601 607 CHEM\n",
      "amphiregulin 610 622 CHEM\n",
      "semaphorin 624 634 CHEM\n",
      "calcitonin 639 649 CHEM\n",
      "CGRP 672 676 CHEM\n",
      "CGRP-R 696 702 CHEM\n",
      "nerve 705 710 ANATOMY\n",
      "NGF 726 729 CHEM\n",
      "TrkA 748 752 CHEM\n",
      "substance P 758 769 CHEM\n",
      "SP 771 773 CHEM\n",
      "SP 792 794 CHEM\n",
      "skin 989 993 ANATOMY\n",
      "biopsy specimens 994 1010 ANATOMY\n",
      "atopic dermatitis 1030 1047 DISO\n",
      "psoriasis 1052 1061 DISO\n",
      "amphiregulin 1094 1106 CHEM\n",
      "NGF 1108 1111 CHEM\n",
      "PGP9.5 1117 1123 CHEM\n",
      "epidermal 1158 1167 ANATOMY\n",
      "nerve 1168 1173 ANATOMY\n",
      "nerve 1214 1219 ANATOMY\n",
      "reduction factor 1220 1236 CHEM\n",
      "semaphorin 1237 1247 CHEM\n",
      "atopic dermatitis 1284 1301 DISO\n",
      "psoriasis 1306 1315 DISO\n",
      "CGRP 1347 1351 CHEM\n",
      "CGRP 1356 1360 CHEM\n",
      "SP 1364 1366 CHEM\n",
      "SP 1368 1370 CHEM\n",
      "inflammatory response 1397 1418 DISO\n",
      "atopic dermatitis 1512 1529 DISO\n",
      "psoriasis 1534 1543 DISO\n",
      "epidermal 1584 1593 ANATOMY\n",
      "nerve 1594 1599 ANATOMY\n",
      "itch 1681 1685 DISO\n",
      "neuropeptides 1715 1728 CHEM\n",
      "neurotrophins 1733 1746 CHEM\n",
      "skin 1790 1794 ANATOMY\n",
      "inflammatory response 1795 1816 DISO\n",
      "body's 1903 1909 ANATOMY\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "#  🔟 Test on an example\n",
    "test_text, test_ann = train_examples[1]\n",
    "doc = nlp(test_text)\n",
    "i=0\n",
    "print(\"Entities predicted:\")\n",
    "for ent in doc.ents:\n",
    "    i+=1\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just a simple function to convert data i'll test to data expected by \"ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:22:17.788725Z",
     "iopub.status.busy": "2025-09-07T13:22:17.788029Z",
     "iopub.status.idle": "2025-09-07T13:22:18.935248Z",
     "shell.execute_reply": "2025-09-07T13:22:18.934553Z",
     "shell.execute_reply.started": "2025-09-07T13:22:17.788702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id        text entity_type      spans\n",
      "0  27456563_en  riboflavin        CHEM  1194-1204\n",
      "1  27456563_en      myopia        DISO  1633-1639\n",
      "2  27456563_en      stress        DISO    174-180\n",
      "3  27456563_en      cornea     ANATOMY    188-194\n",
      "4  27456563_en      cornea     ANATOMY    367-373\n",
      "[The possibility of using standardized self-report anxiety and depression scales in elderly patients: anxiety scales/questionnaires].\n",
      "AIM  To describe the specifics of using self-report anxiety scales in elderly patients, determine the parameters of their reliability and validity and develop recommendations on the use of these scales.\n",
      "MATERIAL AND METHODS\n",
      "The study included 234 patients, aged over 50 years, with non-psychotic anxiety disorders.\n",
      "The following scales/questionnaires BAI, GAI, STAI, ZAS, HADS were used at baseline and 12 weeks after treatment.\n",
      "Conditions of testing, form and content of instructions were similar.\n",
      "Data analysis included the estimation of reliability and validity of these scales.\n",
      "Assessment made by the physician on the Hamilton anxiety scale (HAМ-А) was used as «validity standard».\n",
      "RESULTS AND CONCLUSION\n",
      "All scales (BAI, GAI, STAI, ZAS, HADS-А) can be used for screening diagnosis of anxiety in elderly people.\n",
      "The diagnostic sensitivity and specificity of the scales for subjective assessment of anxiety slightly decrease in advanced age and is sufficient for screening purposes.\n",
      "Based on the analysis, the authors have developed recommendations (common and specific for each scale) on the optimization of using self-report anxiety scales in advanced age.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_to_data(path_tsv , path_test):\n",
    "    directory_test = path_test\n",
    "    annotations_path = path_tsv\n",
    "    df = pd.read_csv(annotations_path , sep=\"\\t\")\n",
    "    print(df.head())\n",
    "    id_to_text = {}\n",
    "    for file in os.listdir(directory_test):\n",
    "        #file_path = os.path.join(directory_test , file)\n",
    "        #print(file)\n",
    "        file_id = file.split(\".\")[0]\n",
    "        with open(os.path.join(directory_test , file) , \"r\") as f:\n",
    "            id_to_text[file_id] = f.read()\n",
    "    #print(id_to_text[\"27456563_en\"])\n",
    "    test_data = []\n",
    "    for  doc_id,group in df.groupby(\"document_id\"):\n",
    "        if doc_id not in id_to_text:\n",
    "            continue\n",
    "\n",
    "        text = id_to_text[doc_id]\n",
    "        \n",
    "        # for _,row in group.iterrows():\n",
    "        #     start , end = row[\"span\"].split(\"-\")\n",
    "        #     start , end = int(start) , int(end)\n",
    "        #     label = row[\"entity_type\"]\n",
    "        #     entities.append((start  , end , label))\n",
    "\n",
    "        test_data.append((text))\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "path_csv = \"/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/tsv/en/bionnel_en_test.tsv\"\n",
    "path_test  =\"/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/texts/en/test\"\n",
    "\n",
    "test_data = convert_to_data(path_csv , path_test)\n",
    "        \n",
    "\n",
    "print(test_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:22:30.316808Z",
     "iopub.status.busy": "2025-09-07T13:22:30.316560Z",
     "iopub.status.idle": "2025-09-07T13:22:30.321240Z",
     "shell.execute_reply": "2025-09-07T13:22:30.320507Z",
     "shell.execute_reply.started": "2025-09-07T13:22:30.316791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-07T13:12:13.738933Z",
     "iopub.status.idle": "2025-09-07T13:12:13.739217Z",
     "shell.execute_reply": "2025-09-07T13:12:13.739121Z",
     "shell.execute_reply.started": "2025-09-07T13:12:13.739110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this s a true test on data he has never seen\n",
    "and the results were good he misses some entities but still it detected 34 out of 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:23:29.879923Z",
     "iopub.status.busy": "2025-09-07T13:23:29.879230Z",
     "iopub.status.idle": "2025-09-07T13:23:29.908171Z",
     "shell.execute_reply": "2025-09-07T13:23:29.907589Z",
     "shell.execute_reply.started": "2025-09-07T13:23:29.879889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted entities for the first text\n",
      "Riboflavin | CHEM\n",
      "cross | ANATOMY\n",
      "cornea | ANATOMY\n",
      "cornea | ANATOMY\n",
      "cornea | ANATOMY\n",
      "cross | ANATOMY\n",
      "vivo | ANATOMY\n",
      "eyes | ANATOMY\n",
      "eyes | ANATOMY\n",
      "myopia | CHEM\n",
      "corneal | ANATOMY\n",
      "samples | ANATOMY\n",
      "TN1S | CHEM\n",
      "machine | CHEM\n",
      "VisuMax | CHEM\n",
      "WaveLight-FS200 | CHEM\n",
      "vivo | ANATOMY\n",
      "cross | ANATOMY\n",
      "cornea | ANATOMY\n",
      "Corneal | ANATOMY\n",
      "corneal | ANATOMY\n",
      "samples | ANATOMY\n",
      "load | CHEM\n",
      "MPa | CHEM\n",
      "MPa | CHEM\n",
      "mild to moderate | DISO\n",
      "riboflavin photoprotection | DISO\n",
      "fellow eye | ANATOMY\n",
      "cornea | ANATOMY\n",
      "cornea | ANATOMY\n",
      "preliminary | ANATOMY\n",
      "stroma | ANATOMY\n",
      "riboflavin | ANATOMY\n",
      "cross | ANATOMY\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = test_data[0]\n",
    "print(\"predicted entities for the first text\")\n",
    "doc = nlp(text)\n",
    "i=0\n",
    "for ent in doc.ents:\n",
    "    i+=1\n",
    "    print(ent.text , \"|\" , ent.label_)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:23:42.621600Z",
     "iopub.status.busy": "2025-09-07T13:23:42.621302Z",
     "iopub.status.idle": "2025-09-07T13:23:42.990549Z",
     "shell.execute_reply": "2025-09-07T13:23:42.990007Z",
     "shell.execute_reply.started": "2025-09-07T13:23:42.621580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id                       text entity_type      spans  UMLS_CUI\n",
      "0  25726786_en                 depression        DISO    686-696  C0011570\n",
      "1  25726786_en                        MCI        DISO    371-374  C1270972\n",
      "2  25726786_en  Mild cognitive impairment        DISO       1-26  C1270972\n",
      "3  25726786_en                   ischemia        DISO  1249-1257  C0022116\n",
      "4  25726786_en                        CAD        DISO  1130-1133  C1956346\n"
     ]
    }
   ],
   "source": [
    "path_csv = \"/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/tsv/en/bionnel_en_dev.tsv\"\n",
    "path_test  =\"/kaggle/input/bionnl-shared-task/NEREL-BIO-master/BioNNE-L_Shared_Task/data/texts/en/dev\"\n",
    "\n",
    "dev_data = convert_to_data(path_csv , path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another test also it did a good job the results are acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:24:00.389277Z",
     "iopub.status.busy": "2025-09-07T13:24:00.388713Z",
     "iopub.status.idle": "2025-09-07T13:24:00.415537Z",
     "shell.execute_reply": "2025-09-07T13:24:00.414986Z",
     "shell.execute_reply.started": "2025-09-07T13:24:00.389254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted entities for the first text\n",
      "arbidol | CHEM\n",
      "umifenovir | CHEM\n",
      "influenza | DISO\n",
      "placebo | CHEM\n",
      "Arbidol | CHEM\n",
      "umifenovir | CHEM\n",
      "influenza | DISO\n",
      "placebo | CHEM\n",
      "influenza | DISO\n",
      "acute respiratory tract infection | DISO\n",
      "oral | ANATOMY\n",
      "placebo | CHEM\n",
      "umifenovir | CHEM\n",
      "influenza | DISO\n",
      "influenza | DISO\n",
      "influenza | DISO\n",
      "influenza | DISO\n",
      "placebo | CHEM\n",
      "umifenovir | CHEM\n",
      "placebo | CHEM\n",
      "viral | ANATOMY\n",
      "influenza | DISO\n",
      "placebo | CHEM\n",
      "umifenovir | CHEM\n",
      "influenza | DISO\n",
      "appears | ANATOMY\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "text = dev_data[3]\n",
    "print(\"predicted entities for the first text\")\n",
    "doc = nlp(text)\n",
    "i=0\n",
    "for ent in doc.ents:\n",
    "    i+=1\n",
    "    print(ent.text , \"|\" , ent.label_)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T13:24:10.202988Z",
     "iopub.status.busy": "2025-09-07T13:24:10.202491Z",
     "iopub.status.idle": "2025-09-07T13:24:10.403863Z",
     "shell.execute_reply": "2025-09-07T13:24:10.403331Z",
     "shell.execute_reply.started": "2025-09-07T13:24:10.202952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/nerel_bio_ner_model.zip'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# source folder\n",
    "model_dir = \"/kaggle/working/nerel_bio_ner_model\"\n",
    "\n",
    "# zip file path (will create nerel_bio_ner_model.zip in working directory)\n",
    "shutil.make_archive(\"/kaggle/working/nerel_bio_ner_model\", 'zip', model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the working as a zip folder"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8213718,
     "sourceId": 12977143,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
